{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import botocore\n",
    "import sagemaker\n",
    "import sys\n",
    "\n",
    "\n",
    "bucket = ''   # specify a bucket you have access to\n",
    "prefix = 'sagemaker/rcf-benchmarks'\n",
    "execution_role = sagemaker.get_execution_role()\n",
    "\n",
    "\n",
    "# check if the bucket exists\n",
    "try:\n",
    "    boto3.Session().client('s3').head_bucket(Bucket=bucket)\n",
    "except botocore.exceptions.ParamValidationError as e:\n",
    "    print(' specify your S3 bucket'\n",
    "          ' or you gave your bucket an invalid name!')\n",
    "except botocore.exceptions.ClientError as e:\n",
    "    if e.response['Error']['Code'] == '403':\n",
    "        print(\" You don't have permission to access the bucket, {}.\".format(bucket))\n",
    "    elif e.response['Error']['Code'] == '404':\n",
    "        print(\" Your bucket, {malware}, doesn't exist!\".format(bucket))\n",
    "    else:\n",
    "        raise\n",
    "else:\n",
    "    print('Training input/output will be stored in: s3://{}/{}'.format(bucket, prefix))\n",
    "\n",
    "\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.session import Session\n",
    "\n",
    "# S3 bucket for saving code and model artifacts.\n",
    "\n",
    "bucket = Session().default_bucket()\n",
    "\n",
    "# Location to save your custom code in tar.gz format.\n",
    "custom_code_upload_location = 's3://{}/tensorflow_scriptmode_pyping/malware'.format(bucket)\n",
    "\n",
    "# Location where results of model training are saved.\n",
    "model_artifacts_location = 's3://{}/tensorflow_scriptmode_pyping/submission'.format(bucket)\n",
    "\n",
    "# IAM execution role that gives SageMaker access to resources in your AWS account.\n",
    "role = get_execution_role()\n",
    "\n",
    "\n",
    "from sagemaker_tensorflow import sequential \n",
    "\n",
    "\n",
    "# Simple example data - a labeled vector.\n",
    "features = {\n",
    "    'data': tf.FixedLenFeature([], tf.string),\n",
    "    'labels': tf.FixedLenFeature([], tf.int64),\n",
    "}\n",
    "\n",
    "# A function to parse record bytes to a labeled vector record\n",
    "def parse(record):\n",
    "    parsed = tf.parse_single_example(record, features)\n",
    "    return ({\n",
    "        'data': tf.decode_raw(parsed['data'], tf.float64)\n",
    "    }, parsed['labels'])\n",
    "\n",
    "# Construct a `malwareDataset` reading from a 'training' channel, using the TF Record encoding.\n",
    "\n",
    "ds = `malwareDataset`(channel='training', record_format='TFRecord')\n",
    "\n",
    "# The `malwareDataset` is a TensorFlow Dataset and provides standard Dataset methods\n",
    "ds = ds.repeat(20)\n",
    "ds = ds.prefetch(10)\n",
    "ds = ds.map(parse, num_parallel_calls=10)\n",
    "ds = ds.batch(64)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "tensorflow = TensorFlow(entry_point='malwarepipemode.py',\n",
    "                        role=role,\n",
    "                        framework_version='1.12.0',\n",
    "                        input_mode='Pipe',\n",
    "                        output_path=model_artifacts_location,\n",
    "                        code_location=custom_code_upload_location,\n",
    "                        train_instance_count=1,\n",
    "                        py_version='py3',\n",
    "                        train_instance_type='ml.c4.xlarge')\n",
    "\n",
    "\n",
    "\n",
    "%%time\n",
    "import boto3\n",
    "\n",
    "# use the region-specific sample data bucket\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "train_data = 's3://sagemaker-sample-data-{}/tensorflow/malware/train'.format(region)\n",
    "eval_data = 's3://sagemaker-sample-data-{}/tensorflow/malware/eval'.format(region)\n",
    "\n",
    "tensorflow.fit({'train':train_data, 'eval':eval_data})\n",
    "\n",
    "from sagemaker import RandomCutForest\n",
    "\n",
    "session = sagemaker.Session()\n",
    "\n",
    "# specify general training job information\n",
    "rcf = RandomCutForest(role=execution_role,\n",
    "                      train_instance_count=1,\n",
    "                      train_instance_type='ml.m4.xlarge',\n",
    "                      data_location='s3://{}/{}/'.format(bucket, prefix),\n",
    "                      output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "                      num_samples_per_tree=512,\n",
    "                      num_trees=50)\n",
    "\n",
    "# automatically upload the training data to S3 and run the training job\n",
    "rcf.fit(rcf.record_set(taxi_data.value.as_matrix().reshape(-1,1)))\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization, Activation\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "# Deploying the Neural network model into Sage Maker \n",
    "\n",
    "#SPLIT TRAIN AND VALIDATION SET\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(\n",
    "    train[cols], train['HasDetections'], test_size = 0.5)\n",
    "\n",
    "# BUILD MODEL\n",
    "model = Sequential()\n",
    "model.add(Dense(100,input_dim=len(cols)))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(100))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer=Adam(lr=0.01), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "annealer = LearningRateScheduler(lambda x: 1e-2 * 0.95 ** x)\n",
    "\n",
    "# TRAIN MODEL\n",
    "model.fit(X_train,Y_train, batch_size=32, epochs = 20, callbacks=[annealer,\n",
    "        printAUC(X_train, Y_train)], validation_data = (X_val,Y_val), verbose=2)\n",
    "\n",
    "optimiser = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cross_entropy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
